{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95c6hK3hAQq"
      },
      "source": [
        "# Rekurencyjne Sieci Neuronowe (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG2v3pqwxT0V"
      },
      "source": [
        "Materiały w większości pochodzą od pracowników wydziału matematyki i informatyki."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laVdd5g5hAQu"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "### Importy i Utilsy  (odpalić i schować)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0D3yk7lhAQu"
      },
      "outputs": [],
      "source": [
        "# imports \n",
        "import torch\n",
        "import os\n",
        "import unicodedata\n",
        "import string\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional, List\n",
        "\n",
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "all_letters = string.ascii_letters\n",
        "n_letters = len(all_letters)\n",
        "\n",
        "\n",
        "class ListDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, targets):\n",
        "        \n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        \n",
        "    def __getitem__(self, ind):\n",
        "        \n",
        "        return self.data[ind], self.targets[ind]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    \n",
        "def unicode_to__ascii(s: str) -> str:\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn'\n",
        "                                                                 and c in all_letters)\n",
        "                   \n",
        "\n",
        "def read_lines(filename: str) -> List[str]:\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicode_to__ascii(line) for line in lines]\n",
        "\n",
        "\n",
        "def letter_to_index(letter: str) -> int:\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "\n",
        "def line_to_tensor(line: str) -> torch.Tensor:\n",
        "    tensor = torch.zeros(len(line), n_letters)\n",
        "    for i, letter in enumerate(line):\n",
        "        tensor[i][letter_to_index(letter)] = 1\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maOHB6NZiRgr",
        "outputId": "39f4b40c-eba8-4c0f-855c-aa5b3402938f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-11 11:33:09--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.226.52.36, 13.226.52.128, 13.226.52.90, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.226.52.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  15.3MB/s    in 0.2s    \n",
            "\n",
            "2022-12-11 11:33:10 (15.3 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip data.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRGjkPZ2hAQv"
      },
      "outputs": [],
      "source": [
        "# NOTE: you can change the seed or remove it completely if you like\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "data_dir = 'data/names'\n",
        "\n",
        "# Build the category_lines dictionary, a list of names per language\n",
        "category_lines = {}\n",
        "all_categories = []\n",
        "\n",
        "data = []\n",
        "targets = [] \n",
        "label_to_idx = {}\n",
        "\n",
        "# read each natonality file and process data \n",
        "for label, file_name in enumerate(os.listdir(data_dir)):\n",
        "\n",
        "    label_to_idx[label] = file_name.split('.')[0].lower()\n",
        "    \n",
        "    names = read_lines(os.path.join(data_dir, file_name))\n",
        "    data += [line_to_tensor(name) for name in names]\n",
        "    targets += len(names) * [label]\n",
        "\n",
        "# split into train and test indices\n",
        "test_frac = 0.1\n",
        "n_test = int(test_frac * len(targets))\n",
        "test_ind = np.random.choice(len(targets), size=n_test, replace=False)\n",
        "train_ind = np.setdiff1d(np.arange(len(targets)), test_ind)\n",
        "\n",
        "targets = torch.tensor(targets)\n",
        "train_targets = targets[train_ind]\n",
        "\n",
        "# calculate weights for BalancedSampler\n",
        "uni, counts = np.unique(train_targets, return_counts=True)\n",
        "weight_per_class = len(targets) / counts\n",
        "weight = [weight_per_class[c] for c in train_targets]\n",
        "# preapre the sampler\n",
        "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights=weight, num_samples=len(weight)) \n",
        "\n",
        "train_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in train_ind], targets=train_targets)\n",
        "train_loader = DataLoader(train_dataset, shuffle=False, batch_size=1, sampler=sampler)\n",
        "\n",
        "test_dataset = ListDataset(data=[x for i, x in enumerate(data) if i in test_ind], targets=targets[test_ind])\n",
        "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvstu1-sldC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9eec2db-02a6-4628-ae29-103125be9890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.shape: torch.Size([1, 15, 52])\n",
            "name: Paraskevopoulos\n",
            "y: greek\n"
          ]
        }
      ],
      "source": [
        "# check out the content of the dataset\n",
        "for i, (x, y) in enumerate(train_loader):\n",
        "    break\n",
        "\n",
        "print(\"x.shape:\", x.shape)\n",
        "print(\"name: \", end=\"\")\n",
        "for letter_onehot in x[0]:\n",
        "    print(all_letters[torch.argmax(letter_onehot)], end=\"\")\n",
        "\n",
        "print(\"\\ny:\", label_to_idx[y.item()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "e-tk2kkRX5yp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3VdtPOhhAQw"
      },
      "source": [
        "<h4> Zadanie 1. </h4>\n",
        "\n",
        "Zaimplementuj \"zwykłą\" sieć rekurencyjną. \n",
        "![rnn](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
        "\n",
        "Przyjmijmy, że stan ukryty ma wymiarowość $H$. Wtedy komórka powinna być warstwą liniową o postaci: $\\tanh(W^T [x_t, h_{t-1}] + b)$, gdzie $x_t \\in \\mathbb{R}^{D}$ to wejście w kroku $t$, $h_{t-1} \\in \\mathbb{R}^{H}$ to stan ukryty z poprzedniego kroku a $W \\in \\mathbb{R}^{(H + D) \\times H}$ i $b \\in \\mathbb{R}^H$ to parametry naszego modelu.\n",
        "\n",
        "* W klasie `RNN` należy zainicjalizować potrzebne wagi oraz zaimplementować główną logikę dla pojedynczej chwili czasowej $x_t$\n",
        "* Wyjście z sieci może mieć dowolny rozmiar, potrzebna jest również warstwa przekształacjąca $H$-wymiarowy stan ukryty na wyjście (o takiej wymiarowości ile mamy klas w naszym problemie). \n",
        "* W pętli uczenia należy dodać odpowiednie wywołanie sieci. HINT: pamiętać o iterowaniu po wymiarze \"czasowym\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNu0vccJhAQw"
      },
      "outputs": [],
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 input_size: int,\n",
        "                 hidden_size: int, \n",
        "                 output_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        :param output_size: int\n",
        "            Desired dimensionality of the output vector\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.input_to_hidden = torch.nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "        self.hidden_to_output = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    \n",
        "    # for the sake of simplicity a single forward will process only a single timestamp \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                hidden: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        :param hidden: torch.tensor\n",
        "            Representation of the memory of the RNN from previous timestep\n",
        "            shape [batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "\n",
        "        combined = torch.cat([input, hidden], dim=1) \n",
        "        hidden = self.input_to_hidden(combined)\n",
        "        output =  self.hidden_to_output(hidden)\n",
        "\n",
        "        return output, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden state\n",
        "        \"\"\"\n",
        "        return torch.zeros(batch_size, self.hidden_size, requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIe3L-8LhAQw"
      },
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXEsqqvxhAQx",
        "outputId": "88685b7e-ee39-45b7-c4f3-c998c5c8b85b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Progress:  0% Loss: 2.869\n",
            "Epoch: 0 Progress:  6% Loss: 2.863\n",
            "Epoch: 0 Progress: 11% Loss: 2.767\n",
            "Epoch: 0 Progress: 17% Loss: 2.486\n",
            "Epoch: 0 Progress: 22% Loss: 2.221\n",
            "Epoch: 0 Progress: 28% Loss: 2.071\n",
            "Epoch: 0 Progress: 33% Loss: 1.926\n",
            "Epoch: 0 Progress: 39% Loss: 1.904\n",
            "Epoch: 0 Progress: 44% Loss: 1.839\n",
            "Epoch: 0 Progress: 50% Loss: 1.793\n",
            "Epoch: 0 Progress: 55% Loss: 1.803\n",
            "Epoch: 0 Progress: 61% Loss: 1.731\n",
            "Epoch: 0 Progress: 66% Loss: 1.716\n",
            "Epoch: 0 Progress: 72% Loss: 1.726\n",
            "Epoch: 0 Progress: 77% Loss: 1.759\n",
            "Epoch: 0 Progress: 83% Loss: 1.611\n",
            "Epoch: 0 Progress: 89% Loss: 1.666\n",
            "Epoch: 0 Progress: 94% Loss: 1.710\n",
            "Epoch: 0 Progress: 100% Loss: 1.736\n",
            "Final F1 score: 0.17\n"
          ]
        }
      ],
      "source": [
        "n_class = len(label_to_idx)\n",
        "\n",
        "# initialize network and optimizer\n",
        "rnn = RNN(n_letters, 256, n_class)\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.01)   \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epochs = 1\n",
        "\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):  \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        # get initial hidden state\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "\n",
        "        # get output for the sample, remember that we treat it as a sequence\n",
        "        # so you need to iterate over the 2nd, time dimensiotn\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        for ix in range(seq_len):       \n",
        "          output, hidden = rnn(x[:,ix,:], hidden) \n",
        "\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()  \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "    \n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    ps = []\n",
        "    ys = []\n",
        "    correct = 0\n",
        "    for i, (x, y) in enumerate(test_loader):\n",
        "        ys.append(y.numpy())\n",
        "\n",
        "        hidden = rnn.init_hidden(x.shape[0])\n",
        "        seq_len = x.shape[1]\n",
        "        \n",
        "        for ix in range(seq_len):        \n",
        "          output, hidden = rnn(x[:,ix,:], hidden) \n",
        "        \n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.15, \"You should get over 0.15 f1 score, try changing some hyperparams!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNeNU93qn7BC"
      },
      "source": [
        "<h4> Zadanie 2. </h4> \n",
        "\n",
        "Zaimplementuj funkcje `predict`, która przyjmuje nazwisko w postaci stringa oraz model RNN i wypisuje 3 najlepsze predykcje narodowości dla tego nazwiska razem z ich logitami.\n",
        "\n",
        "**Hint**: Przyda się tutaj jedna z funkcji z pierwszej komórki notebooka."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8FhF_08hAQy"
      },
      "outputs": [],
      "source": [
        "def predict(name: str, rnn: RNN):\n",
        "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
        "\n",
        "    hidden_state = rnn.init_hidden(1)\n",
        "    tensorified = line_to_tensor(name).unsqueeze(1)\n",
        "\n",
        "\n",
        "    for i in range(len(name)):\n",
        "        input = tensorified[i]\n",
        "        output, hidden = rnn(input, hidden_state)\n",
        "        hidden_state = hidden\n",
        "\n",
        "\n",
        "    top_logits, top_indices = output.topk(3, dim = 1) \n",
        "    top_logits = torch.squeeze(top_logits)\n",
        "    top_indices = torch.squeeze(top_indices)\n",
        "\n",
        "\n",
        "    for logit, index in zip(top_logits, top_indices):\n",
        "        label = label_to_idx[index.item()]\n",
        "        print(f\"{label}:  {logit.item():.3f}\")\n",
        "\n",
        "\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4OWP8wqhAQy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c3f589-3349-4686-8bbd-64a75539850f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satoshi\n",
            "japanese:  3.679\n",
            "polish:  2.260\n",
            "arabic:  1.983\n",
            "Jackson\n",
            "scottish:  3.843\n",
            "english:  1.783\n",
            "greek:  1.703\n",
            "Schmidhuber\n",
            "german:  3.671\n",
            "dutch:  2.541\n",
            "english:  1.817\n",
            "Hinton\n",
            "scottish:  3.487\n",
            "english:  2.109\n",
            "russian:  1.444\n",
            "Kowalski\n",
            "polish:  5.247\n",
            "russian:  4.052\n",
            "czech:  3.765\n"
          ]
        }
      ],
      "source": [
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "\n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict(name, rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNETvP06hAQz"
      },
      "source": [
        "<h4> Zadanie 3 </h4>\n",
        "\n",
        "Ostatnim zadaniem jest implementacji komórki i sieci LSTM. \n",
        "\n",
        "![lstm](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "* W klasie `LSTMCell` ma znaleźć się główna logika LSTMa, czyli wszystkie wagi do stanów `hidden` i `cell` jak i bramek kontrolujących te stany. \n",
        "* W klasie `LSTM` powinno znaleźć się wywołanie komórki LSTM, HINT: poprzednio było w pętli uczenia, teraz przenisiemy to do klasy modelu.\n",
        "* W pętli uczenia należy uzupełnić brakujące wywołania do uczenia i ewaluacji modelu.\n",
        "\n",
        "Zdecydowanie polecam [materiały Chrisa Olaha](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) do zarówno zrozumienia jak i ściągi do wzorów.\n",
        "\n",
        "Zadaniem jest osiągnięcie wartości `f1_score` lepszej niż na sieci RNN, przy prawidłowej implementacji nie powinno być z tym problemów używając podanych hiperparametrów. Dozwolona jest oczywiście zmiana `random seed`.\n",
        "\n",
        "#### Komórka LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNKRxYwChAQz"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTMCell, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # initialize LSTM weights \n",
        "        # NOTE: there are different approaches that are all correct \n",
        "        # (e.g. single matrix for all input opperations), you can pick\n",
        "        # whichever you like for this task\n",
        "    \n",
        "        self.weights = {\n",
        "            'xi': torch.nn.Parameter(torch.Tensor(input_size, hidden_size)),\n",
        "            'xf': torch.nn.Parameter(torch.Tensor(input_size, hidden_size)),\n",
        "            'xo': torch.nn.Parameter(torch.Tensor(input_size, hidden_size)),\n",
        "            'xg': torch.nn.Parameter(torch.Tensor(input_size, hidden_size)),\n",
        "            'hi': torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size)),\n",
        "            'hf': torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size)),\n",
        "            'ho': torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size)),\n",
        "            'hg': torch.nn.Parameter(torch.Tensor(hidden_size, hidden_size)),\n",
        "        }\n",
        "        \n",
        "\n",
        "        #zdecydowałem się zaimplementować modyfikację LSTM z biasami\n",
        "        self.biases = {\n",
        "            'i': torch.nn.Parameter(torch.Tensor(hidden_size)),\n",
        "            'f': torch.nn.Parameter(torch.Tensor(hidden_size)),\n",
        "            'o': torch.nn.Parameter(torch.Tensor(hidden_size)),\n",
        "            'g': torch.nn.Parameter(torch.Tensor(hidden_size)),\n",
        "        }\n",
        "\n",
        "        #initailizing with small values - didnt work with torch.zeros\n",
        "        for key in self.weights:\n",
        "          torch.nn.init.xavier_normal_(self.weights[key])\n",
        "        for key in self.biases:\n",
        "          self.biases[key] = torch.nn.Parameter(torch.randn(hidden_size) * 0.01)\n",
        "\n",
        "        \n",
        "    def forward(self, \n",
        "                input: torch.tensor, \n",
        "                states: Tuple[torch.tensor, torch.tensor]) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \n",
        "        hidden, cell = states\n",
        "\n",
        "        # Compute input, forget, and output gates\n",
        "        # then compute new cell state and hidden state\n",
        "        # see http://colah.github.io/posts/2015-08-Understanding-LSTMs/   \n",
        "\n",
        "\n",
        "        i = torch.sigmoid(torch.matmul(input, self.weights['xi']) + torch.matmul(hidden, self.weights['hi']) + self.biases['i'])\n",
        "        f = torch.sigmoid(torch.matmul(input, self.weights['xf']) + torch.matmul(hidden, self.weights['hf'])  + self.biases['f'])\n",
        "        o = torch.sigmoid(torch.matmul(input, self.weights['xo']) + torch.matmul(hidden, self.weights['ho']) + self.biases['o'])\n",
        "        g = torch.tanh(torch.matmul(input, self.weights['xg']) + torch.matmul(hidden, self.weights['hg']) + self.biases['g'])\n",
        "\n",
        "        cell = f * cell + i * g\n",
        "        hidden = o * torch.tanh(cell)\n",
        "\n",
        "        return hidden, cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U5U8kizhAQz"
      },
      "source": [
        "### Klasa modelu LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2MyIu3_hAQz"
      },
      "outputs": [],
      "source": [
        "class LSTM(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, \n",
        "                 input_size: int, \n",
        "                 hidden_size: int):\n",
        "        \"\"\"\n",
        "        :param input_size: int\n",
        "            Dimensionality of the input vector\n",
        "        :param hidden_size: int\n",
        "            Dimensionality of the hidden space\n",
        "        \"\"\"\n",
        "        \n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.cell = LSTMCell(input_size=self.input_size, hidden_size=self.hidden_size)   \n",
        "             \n",
        "    def forward(self, \n",
        "                input: torch.tensor) -> Tuple[torch.tensor, torch.tensor]:\n",
        "        \"\"\"\n",
        "        :param input: torch.tensor \n",
        "            Input tesnor for a single observation at timestep t\n",
        "            shape [batch_size, input_size]\n",
        "        Returns Tuple of two torch.tensors, both of shape [seq_len, batch_size, hidden_size]\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = input.shape[0]\n",
        "        \n",
        "        hidden, cell = self.init_hidden_cell(batch_size)\n",
        "        \n",
        "        hiddens = []\n",
        "        cells = []\n",
        "        \n",
        "        # this time we will process the whole sequence in the forward method\n",
        "        # as oppose to the previous exercise, remember to loop over the timesteps\n",
        "        \n",
        "        seq_len = input.shape[1]\n",
        "        \n",
        "        for ix in range(seq_len):\n",
        "            hidden, cell = self.cell(input=input[:, ix, :], states=(hidden, cell))\n",
        "            hiddens.append(hidden)\n",
        "            cells.append(cell)\n",
        "\n",
        "\n",
        "        return torch.stack(hiddens), torch.stack(cells)\n",
        "    \n",
        "    def init_hidden_cell(self, batch_size):\n",
        "        \"\"\"\n",
        "        Returns initial value for the hidden and cell states\n",
        "        \"\"\"\n",
        "        return (torch.zeros(batch_size, self.hidden_size, requires_grad=True), \n",
        "                torch.zeros(batch_size, self.hidden_size, requires_grad=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qRxPI-nhAQz"
      },
      "source": [
        "### Pętla uczenia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LVCWqsVhAQ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7b19062-b1f7-4f97-9388-107b0a461c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Progress:  0% Loss: 2.866\n",
            "Epoch: 0 Progress:  6% Loss: 2.486\n",
            "Epoch: 0 Progress: 11% Loss: 2.196\n",
            "Epoch: 0 Progress: 17% Loss: 2.045\n",
            "Epoch: 0 Progress: 22% Loss: 1.999\n",
            "Epoch: 0 Progress: 28% Loss: 1.862\n",
            "Epoch: 0 Progress: 33% Loss: 1.848\n",
            "Epoch: 0 Progress: 39% Loss: 1.825\n",
            "Epoch: 0 Progress: 44% Loss: 1.837\n",
            "Epoch: 0 Progress: 50% Loss: 1.831\n",
            "Epoch: 0 Progress: 55% Loss: 1.803\n",
            "Epoch: 0 Progress: 61% Loss: 1.773\n",
            "Epoch: 0 Progress: 66% Loss: 1.715\n",
            "Epoch: 0 Progress: 72% Loss: 1.735\n",
            "Epoch: 0 Progress: 77% Loss: 1.772\n",
            "Epoch: 0 Progress: 83% Loss: 1.774\n",
            "Epoch: 0 Progress: 89% Loss: 1.705\n",
            "Epoch: 0 Progress: 94% Loss: 1.705\n",
            "Epoch: 0 Progress: 100% Loss: 1.658\n",
            "Final F1 score: 0.21\n"
          ]
        }
      ],
      "source": [
        "from itertools import chain\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# build data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "\n",
        "# initialize the lstm with an additional cliassifier layer at the top\n",
        "lstm = LSTM(input_size=len(all_letters), hidden_size=128)\n",
        "\n",
        "clf = torch.nn.Linear(in_features=128, out_features=len(label_to_idx))\n",
        "\n",
        "# initialize a optimizer\n",
        "params = chain(lstm.parameters(), clf.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.015) \n",
        "\n",
        "# we will train for only a single epoch \n",
        "epoch = 1\n",
        "\n",
        "# main loop\n",
        "for epoch in range(epoch):\n",
        "    \n",
        "    loss_buffer = []\n",
        "    \n",
        "    for i, (x, y) in enumerate(train_loader):   \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # don't forget about the classifier!\n",
        "        hidden, state = lstm(x)\n",
        "        last_hidden = hidden[-1]\n",
        "        output = clf(last_hidden)\n",
        "        \n",
        "\n",
        "        # calucate the loss\n",
        "        loss = cross_entropy(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()                                \n",
        "        \n",
        "        loss_buffer.append(loss.item())\n",
        "        \n",
        "        if i % 1000 == 1:\n",
        "            print(f\"Epoch: {epoch} Progress: {100 * i/len(train_loader):2.0f}% Loss: {np.mean(loss_buffer):.3f}\")\n",
        "            loss_buffer = []\n",
        "\n",
        "# evaluate on the test set\n",
        "with torch.no_grad():\n",
        "    \n",
        "    ps = []\n",
        "    ys = []\n",
        "    for i, (x, y) in enumerate(test_loader): \n",
        "        ys.append(y.numpy())\n",
        "        hidden, state = lstm(x)\n",
        "        last_hidden = hidden[-1]\n",
        "        output = clf(last_hidden)\n",
        "        \n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "        ps.append(pred.cpu().numpy())\n",
        "    \n",
        "    ps = np.concatenate(ps, axis=0)\n",
        "    ys = np.concatenate(ys, axis=0)\n",
        "    f1 = f1_score(ys, ps, average='weighted')\n",
        "    \n",
        "    print(f\"Final F1 score: {f1:.2f}\")\n",
        "    assert f1 > 0.18, \"You should get over 0.18 f1 score, try changing some hiperparams!\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Próbowałem zejśc z loss poniżej 1, ale nie ważne ile zmieniałem to jakoś ta moja implementacja się zacina przy 1,7-1,4.\n",
        "Chociaż F!_score jest większy niż dla RNN."
      ],
      "metadata": {
        "id": "s_G-gtk4Xl3o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGXUhgroo7AN"
      },
      "source": [
        "<h4> Zadanie 4. </h4>\n",
        "\n",
        "Zaimplementuj analogiczną do funkcji `predict` z zadania 2 dla modelu `lstm+clf`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ChJv1fphAQ0"
      },
      "outputs": [],
      "source": [
        "def predict_lstm(name: str, lstm: LSTM, clf: torch.nn.Module):\n",
        "    \"\"\"Prints the name and model's top 3 predictions with scores\"\"\"\n",
        "    tensorified = line_to_tensor(name)\n",
        "    tensorified = torch.unsqueeze(tensorified, dim = 0)\n",
        "    hiddens, cells = lstm(tensorified)\n",
        "    logits = clf(hiddens[-1])\n",
        "\n",
        "    top_logits, top_indices = logits.topk(3, dim = 1) \n",
        "\n",
        "    top_logits = torch.squeeze(top_logits, dim = 0)\n",
        "    top_indices = torch.squeeze(top_indices, dim = 0)\n",
        "\n",
        "\n",
        "    for logit, index in zip(top_logits, top_indices):\n",
        "        label = label_to_idx[index.item()]\n",
        "        print(f\"{label}:  {logit.item():.3f}\")\n",
        "\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgQcGWqthAQ0",
        "outputId": "11e05d78-fcfc-4001-96a7-8e522f6b5385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Satoshi\n",
            "japanese:  2.567\n",
            "arabic:  1.889\n",
            "italian:  0.791\n",
            "Jackson\n",
            "scottish:  1.095\n",
            "russian:  0.193\n",
            "korean:  -0.031\n",
            "Schmidhuber\n",
            "german:  2.808\n",
            "french:  1.340\n",
            "arabic:  1.339\n",
            "Hinton\n",
            "scottish:  0.838\n",
            "english:  0.520\n",
            "russian:  0.438\n",
            "Kowalski\n",
            "polish:  2.795\n",
            "japanese:  0.886\n",
            "greek:  0.599\n"
          ]
        }
      ],
      "source": [
        "# test your lstm predictor\n",
        "some_names = [\"Satoshi\", \"Jackson\", \"Schmidhuber\", \"Hinton\", \"Kowalski\"]\n",
        "    \n",
        "for name in some_names:\n",
        "    print(name)\n",
        "    predict_lstm(name, lstm, clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Niestety nie udało mi się zejść do tak dużych różnic pomiędzy pierwszym a drugim miejscem, ale przynajmniej jęzeyki na pierwszym miejscu się zgadzają"
      ],
      "metadata": {
        "id": "L-iaY4m-YDXx"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}